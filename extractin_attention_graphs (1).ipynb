{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeaa26d-5280-4a26-858d-8e9f6b072f11",
   "metadata": {},
   "source": [
    "# Extracting All Attention Graphs from a Transformer in English and Hebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b226c63-ab05-4cbf-a108-6a46155937c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a8481-0c5f-4f8b-a018-a3e44c1e94e4",
   "metadata": {},
   "source": [
    "Here I just used the following prompt:\n",
    "\n",
    "\"I am interested in extracting attention graphs from the transformer model GPT-2 for a given text called `input_text`. Please choose an input text and help me write code to extract the attention graphs for it. Each attention graph should be plotted with weight lables on edges and node labels.\"\n",
    "\n",
    "This took a little conversation warmup to obtain working code, but once the code ran, it was easy to check visually that it was working using the attention graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2889e2d-8e16-451c-a7d0-cf3ab8188b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Set up the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the input text and convert it to a tensor\n",
    "input_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)\n",
    "\n",
    "# Get the model output with attention weights\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Iterate through each layer of the output and extract the attention weights\n",
    "for layer, attention in enumerate(outputs.attentions):\n",
    "    # Convert the attention weights to a weighted adjacency matrix\n",
    "    weights = attention[0].detach().numpy()  # Shape: (num_heads, sequence_length, sequence_length)\n",
    "    for head in range(weights.shape[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        sequence_length = weights.shape[1]\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(sequence_length):\n",
    "                graph.add_edge(i, j, weight=weights[head][i][j])\n",
    "        # Plot the graph with edge and node labels\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_edges(graph, pos, width=2, edge_color='grey')\n",
    "        edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color='lightblue', node_size=2000)\n",
    "        node_labels = {i: tokenizer.decode([input_ids[0][i].item()]) for i in range(sequence_length)}\n",
    "        nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n",
    "        plt.title(f\"Layer {layer+1} Head {head+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa38d23-fccc-4300-a54a-a65827f8bbe6",
   "metadata": {},
   "source": [
    "The first example doesn't work properly, as we can see from the attention graphs. It seems the model learns the unicode encoding of the Hebrew and then forms the attention graphs (using a sliding window of 12 tokens presumably).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627976a8-6013-4114-a814-23f976ad39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Set up the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"שְׁמַע יִשְׂרָאֵל יְהוָה אֱלֹהֵינוּ יְהוָה אֶחָד\"\n",
    "\n",
    "# Tokenize the input text and convert it to a tensor\n",
    "tokens = torch.tensor(tokenizer.encode(input_text, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "# Get the model output with attention weights\n",
    "outputs = model(tokens)\n",
    "\n",
    "# Iterate through each layer of the output and extract the attention weights\n",
    "for layer, attention in enumerate(outputs.attentions):\n",
    "    # Convert the attention weights to a weighted adjacency matrix\n",
    "    weights = attention[0].detach().numpy()  # Shape: (num_heads, sequence_length, sequence_length)\n",
    "    for head in range(weights.shape[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        sequence_length = weights.shape[1]\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(sequence_length):\n",
    "                graph.add_edge(i, j, weight=weights[head][i][j])\n",
    "        # Plot the graph with edge and node labels\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_edges(graph, pos, width=2, edge_color='grey')\n",
    "        edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color='lightblue', node_size=2000)\n",
    "        node_labels = {i: tokenizer.decode([tokens[0][i].item()]) for i in range(sequence_length)}\n",
    "        nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n",
    "        plt.title(f\"Layer {layer+1} Head {head+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e0e2e-86ec-478b-ad64-55535bda859c",
   "metadata": {},
   "source": [
    "Next, I tried having it use a different model to parse the Hebrew. This worked well and didn't seem to need any special treatment of the Nikkud. When At this point it began struggling to translate from Hebrew to Yiddish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b889f-9dcd-4d93-ba6f-df10a6a09360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set up the GPT-2 model and tokenizer\n",
    "model_name = 'onlplab/alephbert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"שְׁמַע יִשְׂרָאֵל יְהוָה אֱלֹהֵינוּ יְהוָה אֶחָד\"\n",
    "\n",
    "# Tokenize the input text and convert it to a tensor\n",
    "input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "# Get the model output with attention weights\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Iterate through each layer of the output and extract the attention weights\n",
    "for layer, attention in enumerate(outputs.attentions):\n",
    "    # Convert the attention weights to a weighted adjacency matrix\n",
    "    weights = attention[0].detach().numpy()  # Shape: (num_heads, sequence_length, sequence_length)\n",
    "    for head in range(weights.shape[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        sequence_length = weights.shape[1]\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(sequence_length):\n",
    "                graph.add_edge(i, j, weight=weights[head][i][j])\n",
    "        # Plot the graph with edge and node labels\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_edges(graph, pos, width=2, edge_color='grey')\n",
    "        edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color='lightblue', node_size=2000)\n",
    "        node_labels = {i: tokenizer.decode([input_ids[0][i].item()]) for i in range(sequence_length)}\n",
    "        nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n",
    "        plt.title(f\"Layer {layer+1} Head {head+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e4b1f-ab31-4d7d-9363-7ed2a27c2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set up the GPT-2 model and tokenizer\n",
    "model_name = 'onlplab/alephbert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"שמע ישראל יהוה אלהינו יהוה אחד\"\n",
    "\n",
    "# Tokenize the input text and convert it to a tensor\n",
    "input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "# Get the model output with attention weights\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Iterate through each layer of the output and extract the attention weights\n",
    "for layer, attention in enumerate(outputs.attentions):\n",
    "    # Convert the attention weights to a weighted adjacency matrix\n",
    "    weights = attention[0].detach().numpy()  # Shape: (num_heads, sequence_length, sequence_length)\n",
    "    for head in range(weights.shape[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        sequence_length = weights.shape[1]\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(sequence_length):\n",
    "                graph.add_edge(i, j, weight=weights[head][i][j])\n",
    "        # Plot the graph with edge and node labels\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_edges(graph, pos, width=2, edge_color='grey')\n",
    "        edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color='lightblue', node_size=2000)\n",
    "        node_labels = {i: tokenizer.decode([input_ids[0][i].item()]) for i in range(sequence_length)}\n",
    "        nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=12)\n",
    "        plt.title(f\"Layer {layer+1} Head {head+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a70ab9-4d17-43b2-8532-58c474b3c5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python385jvsc74a57bd0474c67ce7e36ad5731492349411c4ce02ca5c170a680b2d1efe1eb0325e35fe7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
