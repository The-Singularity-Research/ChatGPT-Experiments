{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340ff503-13cd-4fbd-abda-33f05eebc29d",
   "metadata": {},
   "source": [
    "# Attention Graph Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbef8d-6e8d-492d-9f15-52e6c46cdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "def plot_attention_graph(attention_weights, tokens):\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    for i, token in enumerate(tokens):\n",
    "        G.add_node(token, size=100)\n",
    "        \n",
    "    # Add edges to the graph\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            edge_weight = attention_weights[i,j].item()\n",
    "            if edge_weight > 0.05:  # Threshold the edge weight at 0.05\n",
    "                source_node = tokens[i]\n",
    "                target_node = tokens[j]\n",
    "                G.add_edge(source_node, target_node, weight=edge_weight)\n",
    "                \n",
    "    # Draw the graph using NetworkX\n",
    "    pos = nx.circular_layout(G)\n",
    "    edge_labels = {(u, v): f\"{w:.2f}\" for u, v, w in G.edges(data=\"weight\")}\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=\"#98FB98\", node_size=1000)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=\"#000000\", width=2)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=14, font_family=\"sans-serif\")\n",
    "    \n",
    "    # Display the graph\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "\n",
    "# Encode a sample text and pass it through the model\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the attention weights for each layer and head\n",
    "attentions = outputs.attentions\n",
    "for layer, layer_attentions in enumerate(attentions):\n",
    "    print(f\"Layer {layer + 1}\")\n",
    "    for head, attention in enumerate(layer_attentions):\n",
    "        print(f\"Head {head + 1}\")\n",
    "        plot_attention_graph(attention[0], tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9303e5-2806-40ce-b757-365513061035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define the input text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the attention matrices for each layer and head\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Create a graph for each layer and head\n",
    "for layer, attention_layer in enumerate(attentions):\n",
    "    for head, attention_head in enumerate(attention_layer[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        for i, from_token in enumerate(tokenizer.tokenize(text)):\n",
    "            for j, to_token in enumerate(tokenizer.tokenize(text)):\n",
    "                weight = attention_head[i][j].item()\n",
    "                if weight > 0.05:\n",
    "                    graph.add_edge(from_token, to_token, weight=weight)\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_size=500)\n",
    "        nx.draw_networkx_labels(graph, pos, labels={i: i for i in graph.nodes()}, font_size=12)\n",
    "        edges = nx.draw_networkx_edges(graph, pos, edge_color=[graph[u][v]['weight'] for u, v in graph.edges()], width=2, arrowstyle='->', arrowsize=10)\n",
    "        edge_labels = nx.draw_networkx_edge_labels(graph, pos, edge_labels={(u, v): f\"{graph[u][v]['weight']:.2f}\" for u, v in graph.edges()}, font_size=12)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Layer {layer + 1}, Head {head + 1}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad521e0b-ceb5-4435-b19b-6f4021ae37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd1e90-249d-4a51-9bd6-6cd23c0ac372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained Hebrew model and tokenizer\n",
    "model_name = \"onlplab/alephbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define the input text\n",
    "text = \"הער, ישראל יהוה אונדזער גאט יהוה איז אײנער.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the attention matrices for each layer and head\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Create a graph for each layer and head\n",
    "for layer, attention_layer in enumerate(attentions):\n",
    "    for head, attention_head in enumerate(attention_layer[0]):\n",
    "        graph = nx.DiGraph()\n",
    "        for i, from_token in enumerate(tokenizer.tokenize(text)):\n",
    "            from_token_display = get_display(from_token[::-1])  # reverse and apply bidi algorithm\n",
    "            graph.add_node(from_token_display)\n",
    "            for j, to_token in enumerate(tokenizer.tokenize(text)):\n",
    "                to_token_display = get_display(to_token[::-1])  # reverse and apply bidi algorithm\n",
    "                weight = attention_head[i][j].item()\n",
    "                if weight > 0.05:\n",
    "                    graph.add_edge(from_token_display, to_token_display, weight=weight)\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        pos = nx.circular_layout(graph)\n",
    "        nx.draw_networkx_nodes(graph, pos, node_size=500)\n",
    "        nx.draw_networkx_labels(graph, pos, labels={node: node[::-1] for node in graph.nodes()}, font_size=12)\n",
    "        edges = nx.draw_networkx_edges(graph, pos, edge_color=[graph[u][v]['weight'] for u, v in graph.edges()], width=2, arrowstyle='->', arrowsize=10)\n",
    "        edge_labels = nx.draw_networkx_edge_labels(graph, pos, edge_labels={(u, v): f\"{graph[u][v]['weight']:.2f}\" for u, v in graph.edges()}, font_size=12)\n",
    "        for label in edge_labels.values():\n",
    "            label.set_rotation(0)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Layer {layer + 1}, Head {head + 1}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad57784-056e-4979-847c-35cd2a62143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def plot_attention_graph(attention_weights, tokens):\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    for i, token in enumerate(tokens):\n",
    "        G.add_node(token, size=100)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            edge_weight = attention_weights[i, j].item()\n",
    "            if edge_weight > 0.05:  # Threshold the edge weight at 0.05\n",
    "                source_node = tokens[i]\n",
    "                target_node = tokens[j]\n",
    "                G.add_edge(source_node, target_node, weight=edge_weight)\n",
    "\n",
    "    # Draw the graph using NetworkX\n",
    "    pos = nx.circular_layout(G)\n",
    "    edge_labels = {(u, v): f\"{w:.2f}\" for u, v, w in G.edges(data=\"weight\")}\n",
    "    edge_colors = [w for _, _, w in G.edges(data=\"weight\")]\n",
    "    node_labels = {n: n if len(n) == 1 else f\" {n} \" for n in G.nodes}\n",
    "    node_label_pos = {k: [v[0], v[1] - 0.05] for k, v in pos.items()}\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=\"#98FB98\", node_size=1000)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=2)\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G,\n",
    "        pos,\n",
    "        edge_labels=edge_labels,\n",
    "        font_size=12,\n",
    "        label_pos=0.3,\n",
    "        rotate=False,\n",
    "        verticalalignment=\"baseline\",\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.2, facecolor=\"white\", edgecolor=\"none\"),\n",
    "    )\n",
    "    nx.draw_networkx_labels(\n",
    "        G,\n",
    "        node_label_pos,\n",
    "        node_labels,\n",
    "        font_size=14,\n",
    "        font_family=\"sans-serif\",\n",
    "    )\n",
    "    # Draw the self-loops\n",
    "    for node in G.nodes():\n",
    "        pos_node = pos[node]\n",
    "        x, y = pos_node[0], pos_node[1] - 0.1\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=[(node, node)],\n",
    "            width=2,\n",
    "            edge_color=\"k\",\n",
    "            style=\"solid\",\n",
    "            connectionstyle=f\"arc3, rad=-0.1\",\n",
    "        )\n",
    "        weight = edge_labels.get((node, node), None)\n",
    "        if weight:\n",
    "            plt.text(x, y, weight, fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Display the graph\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "\n",
    "# Encode a sample text and pass it through the model\n",
    "text = \"The quick brown fox jumped over the lazy dog\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the attention weights for each layer and head\n",
    "attentions = outputs.attentions\n",
    "for layer, layer_attentions in enumerate(attentions):\n",
    "    print(f\"Layer {layer + 1}\")\n",
    "    for head, attention in enumerate(layer_attentions):\n",
    "        print(f\"Head {head + 1}\")\n",
    "        plot_attention_graph(attention[0], tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952baa84-b945-43f8-9f93-62401cac2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def plot_attention_graph(attention_weights, tokens):\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    for i, token in enumerate(tokens):\n",
    "        G.add_node(token, size=100)\n",
    "        \n",
    "    # Add edges to the graph\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            edge_weight = attention_weights[i,j].item()\n",
    "            if edge_weight > 0.05:  # Threshold the edge weight at 0.05\n",
    "                source_node = tokens[i]\n",
    "                target_node = tokens[j]\n",
    "                G.add_edge(source_node, target_node, weight=edge_weight)\n",
    "    \n",
    "    # Draw the graph using NetworkX\n",
    "    pos = nx.circular_layout(G)\n",
    "    edge_labels = {(u, v): f\"{w:.2f}\" for u, v, w in G.edges(data=\"weight\")}\n",
    "    edge_colors = [w for _, _, w in G.edges(data=\"weight\")]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=\"#98FB98\", node_size=1000)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=2, arrows=True, edge_cmap=plt.cm.Blues)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12, label_pos=0.65, font_color=\"r\")\n",
    "    nx.draw_networkx_labels(G, pos, font_size=14, font_family=\"sans-serif\")\n",
    "    \n",
    "    # Add offset to self-loops\n",
    "    shift = 0.1\n",
    "    for node in G.nodes:\n",
    "        if (node, node) in G.edges:\n",
    "            x, y = pos[node]\n",
    "            plt.text(x+shift, y+shift, s=f\"{G.get_edge_data(node, node)['weight']:.2f}\", bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # Display the graph\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "\n",
    "# Encode a sample text and pass it through the model\n",
    "text = \"The quick brown fox jumped over the lazy dog\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the attention weights for each layer and head\n",
    "attentions = outputs.attentions\n",
    "for layer, layer_attentions in enumerate(attentions):\n",
    "    print(f\"Layer {layer + 1}\")\n",
    "    for head, attention in enumerate(layer_attentions):\n",
    "        print(f\"Head {head + 1}\")\n",
    "        plot_attention_graph(attention[0], tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a368a72-327e-4974-9d45-686009104577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python385jvsc74a57bd0474c67ce7e36ad5731492349411c4ce02ca5c170a680b2d1efe1eb0325e35fe7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
