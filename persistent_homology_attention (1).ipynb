{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e68a401-fa4a-4c84-ae63-0f702d0b9eaa",
   "metadata": {},
   "source": [
    "# Persistent Homology of Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30809e67-7bde-4f5a-9b9a-262b9ecc102a",
   "metadata": {},
   "source": [
    "This code is designed to visualize the attention mechanism of pre-trained transformer models using simplicial complexes. It consists of several functions that handle attention matrix computation, persistence computation, and 3D visualization, as well as interactive widgets to adjust input parameters.\n",
    "\n",
    "The code uses persistent homology to analyze the structure of attention mechanisms in transformer models by visualizing the relationships between tokens as simplicial complexes. Persistent homology is a mathematical method that studies the topological features of a shape across different scales. It provides a measure of the significance of topological features such as connected components, loops, and voids, which helps to understand the underlying structure in data.\n",
    "\n",
    "In this code, persistent homology is used in the following way:\n",
    "\n",
    "1. The `compute_persistence()` function calculates the persistence, simplex tree, and distance matrix based on the attention matrix. The attention matrix is first transformed into a probability distribution using the softmax function. Then, the Jensen-Shannon distance is calculated between each pair of probability distributions, creating a distance matrix.\n",
    "\n",
    "2. The distance matrix is used to construct a Rips complex, which is a simplicial complex built by connecting points within a certain distance threshold. In this code, the Rips complex is created using the `gd.RipsComplex()` function from the Gudhi library.\n",
    "\n",
    "3. The Rips complex is then converted into a simplex tree using the `create_simplex_tree()` method. A simplex tree is a data structure that represents a filtered simplicial complex, which is a simplicial complex where each simplex is associated with a value called its filtration value.\n",
    "\n",
    "4. The persistence of the topological features is computed using the `persistence()` method. Persistence is a measure of the importance of a topological feature based on how long it persists across different scales (filtration values). \n",
    "\n",
    "5. The `plot_simplicial_complex_3d()` function generates a 3D visualization of the simplicial complex based on the simplex tree and distance matrix. The visualization shows the relationships between tokens as edges, with the persistence threshold determining which edges are displayed.\n",
    "\n",
    "By using persistent homology, the code provides a way to study the attention mechanism's structure in transformer models and visualize how tokens are related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ab9adc-4b95-4a3d-9e31-57eaa4a46a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy torch transformers gudhi matplotlib networkx scipy plotly ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee85db1b-906a-497a-bd14-b8973dc5d0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3ebf802c2e4319869b44383ecd563b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.05, continuous_update=False, description='Threshold:', max=0.5, stepâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(threshold, text1, text2, layer, head, model_name)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gudhi as gd\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Text, Dropdown, VBox, Label\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def plot_persistence_diagram(persistence, title):\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_bottleneck_distance(persistence1, persistence2):\n",
    "    persistence1_array = np.array([(birth, death) for dim, (birth, death) in persistence1 if dim == 0])\n",
    "    persistence2_array = np.array([(birth, death) for dim, (birth, death) in persistence2 if dim == 0])\n",
    "    return gd.bottleneck_distance(persistence1_array, persistence2_array)\n",
    "\n",
    "def get_attention_matrix(text, model, tokenizer, layer, head):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attention = outputs.attentions[layer][0, head].detach().cpu().numpy()\n",
    "    return attention\n",
    "\n",
    "def compute_persistence(attention_matrix):\n",
    "    softmax_attention = np.exp(attention_matrix) / np.sum(np.exp(attention_matrix), axis=-1)[:, np.newaxis]\n",
    "    distance_matrix = np.array([[np.sqrt(jensenshannon(softmax_attention[i], softmax_attention[j])) for j in range(softmax_attention.shape[0])] for i in range(softmax_attention.shape[0])])\n",
    "    \n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distance_matrix, max_edge_length=np.inf)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistence = simplex_tree.persistence(min_persistence=0.01)\n",
    "    return persistence, simplex_tree, distance_matrix\n",
    "\n",
    "def plot_simplicial_complex_3d(simplex_tree, distance_matrix, title, threshold, tokens):\n",
    "    g = nx.Graph()\n",
    "    for (simplex, _) in simplex_tree.get_filtration():\n",
    "        if len(simplex) == 2:\n",
    "            if distance_matrix[simplex[0]][simplex[1]] <= threshold:\n",
    "                g.add_edge(simplex[0], simplex[1])\n",
    "\n",
    "    labels = {node: tokens[node] for node in g.nodes()}\n",
    "    \n",
    "    pos = nx.spring_layout(g, dim=3, seed=42)\n",
    "    \n",
    "    Xn = [pos[k][0] for k in g.nodes()]\n",
    "    Yn = [pos[k][1] for k in g.nodes()]\n",
    "    Zn = [pos[k][2] for k in g.nodes()]\n",
    "    \n",
    "    Xe = []\n",
    "    Ye = []\n",
    "    Ze = []\n",
    "    for e in g.edges():\n",
    "        Xe += [pos[e[0]][0], pos[e[1]][0], None]\n",
    "        Ye += [pos[e[0]][1], pos[e[1]][1], None]\n",
    "        Ze += [pos[e[0]][2], pos[e[1]][2], None]\n",
    "    \n",
    "    trace_edges = go.Scatter3d(x=Xe, y=Ye, z=Ze, mode='lines', line=dict(color='gray', width=1))\n",
    "    \n",
    "    trace_nodes = go.Scatter3d(x=Xn, y=Yn, z=Zn, mode='markers+text', text=list(labels.values()), marker=dict(symbol='circle', size=10, color='lightblue'), textposition=\"top center\")\n",
    "    \n",
    "    layout = go.Layout(title=title, scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'), showlegend=False)\n",
    "    \n",
    "    fig = go.Figure(data=[trace_edges, trace_nodes], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "model_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('GPT-2', 'gpt2'),\n",
    "        ('DistilBERT', 'distilbert-base-uncased'),\n",
    "        ('Bert', 'bert-base-uncased'),\n",
    "        ('RoBERTa', 'roberta-base'),\n",
    "        ('aleph-BeRT', 'onlplab/alephbert-base')\n",
    "    ],\n",
    "    value='gpt2',\n",
    "    description='Model:'\n",
    ")\n",
    "\n",
    "tokenizer, model = load_model(model_dropdown.value)\n",
    "\n",
    "text_input1 = Text(description='Text 1:', value='Quantum information theory is fascinating')\n",
    "text_input2 = Text(description='Text 2:', value='Quantum information theory allows us to study attention using entanglement')\n",
    "\n",
    "layer_slider = IntSlider(value=1, min=0, max=model.config.num_hidden_layers - 1, description='Layer:', continuous_update=False)\n",
    "head_slider = IntSlider(value=2, min=0, max=model.config.num_attention_heads - 1, description='Head:', continuous_update=False)\n",
    "\n",
    "threshold_slider = FloatSlider(value=0.05, min=0.00, max=0.5, step=0.001, description='Threshold:', continuous_update=False)\n",
    "\n",
    "# Update the update_plot function\n",
    "def update_plot(threshold, text1, text2, layer, head, model_name):\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    layer_slider.max = model.config.num_hidden_layers - 1\n",
    "    head_slider.max = model.config.num_attention_heads - 1\n",
    "\n",
    "    attention_matrix1 = get_attention_matrix(text1, model, tokenizer, layer, head)\n",
    "    attention_matrix2 = get_attention_matrix(text2, model, tokenizer, layer, head)\n",
    "\n",
    "    persistence1, simplex_tree1, distance_matrix1 = compute_persistence(attention_matrix1)\n",
    "    persistence2, simplex_tree2, distance_matrix2 = compute_persistence(attention_matrix2)\n",
    "\n",
    "    tokens1 = tokenizer.tokenize(text1)\n",
    "    tokens2 = tokenizer.tokenize(text2)\n",
    "\n",
    "    tokens1 = [tokenizer.decode(token_id) for token_id in tokenizer.encode(text1)]\n",
    "    tokens2 = [tokenizer.decode(token_id) for token_id in tokenizer.encode(text2)]\n",
    "\n",
    "    plot_simplicial_complex_3d(simplex_tree1, distance_matrix1, \"Simplicial Complex for Text 1\", threshold, tokens1)\n",
    "    plot_simplicial_complex_3d(simplex_tree2, distance_matrix2, \"Simplicial Complex for Text 2\", threshold, tokens2)\n",
    "    plot_persistence_diagram(persistence1, \"Persistence Diagram for Text 1\")\n",
    "    plot_persistence_diagram(persistence2, \"Persistence Diagram for Text 2\")\n",
    "    # Compute and display bottleneck distance between persistence diagrams\n",
    "    bottleneck_distance = compute_bottleneck_distance(persistence1, persistence2)\n",
    "    print(\"Bottleneck distance between Text 1 and Text 2:\", bottleneck_distance)\n",
    "\n",
    "interact(update_plot, threshold=threshold_slider, text1=text_input1, text2=text_input2, layer=layer_slider, head=head_slider, model_name=model_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5328b1-0005-4c4e-8014-e8c415d426cb",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's run one without the persistence diagrams plotted (and without the bottleneck distance between them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45da6529-419d-4c31-9575-f1f2e0678936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcfde33818343dfa526679196b0f2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.05, continuous_update=False, description='Threshold:', max=0.5, stepâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(threshold, text1, text2, layer, head, model_name)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gudhi as gd\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Text, Dropdown, VBox, Label\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def get_attention_matrix(text, model, tokenizer, layer, head):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attention = outputs.attentions[layer][0, head].detach().cpu().numpy()\n",
    "    return attention\n",
    "\n",
    "def compute_persistence(attention_matrix):\n",
    "    softmax_attention = np.exp(attention_matrix) / np.sum(np.exp(attention_matrix), axis=-1)[:, np.newaxis]\n",
    "    distance_matrix = np.array([[np.sqrt(jensenshannon(softmax_attention[i], softmax_attention[j])) for j in range(softmax_attention.shape[0])] for i in range(softmax_attention.shape[0])])\n",
    "    \n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distance_matrix, max_edge_length=np.inf)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistence = simplex_tree.persistence(min_persistence=0.01)\n",
    "    return persistence, simplex_tree, distance_matrix\n",
    "\n",
    "def plot_simplicial_complex_3d(simplex_tree, distance_matrix, title, threshold, tokens):\n",
    "    g = nx.Graph()\n",
    "    for (simplex, _) in simplex_tree.get_filtration():\n",
    "        if len(simplex) == 2:\n",
    "            if distance_matrix[simplex[0]][simplex[1]] <= threshold:\n",
    "                g.add_edge(simplex[0], simplex[1])\n",
    "\n",
    "    labels = {node: tokens[node] for node in g.nodes()}\n",
    "    \n",
    "    pos = nx.spring_layout(g, dim=3, seed=42)\n",
    "    \n",
    "    Xn = [pos[k][0] for k in g.nodes()]\n",
    "    Yn = [pos[k][1] for k in g.nodes()]\n",
    "    Zn = [pos[k][2] for k in g.nodes()]\n",
    "    \n",
    "    Xe = []\n",
    "    Ye = []\n",
    "    Ze = []\n",
    "    for e in g.edges():\n",
    "        Xe += [pos[e[0]][0], pos[e[1]][0], None]\n",
    "        Ye += [pos[e[0]][1], pos[e[1]][1], None]\n",
    "        Ze += [pos[e[0]][2], pos[e[1]][2], None]\n",
    "    \n",
    "    trace_edges = go.Scatter3d(x=Xe, y=Ye, z=Ze, mode='lines', line=dict(color='gray', width=1))\n",
    "    \n",
    "    trace_nodes = go.Scatter3d(x=Xn, y=Yn, z=Zn, mode='markers+text', text=list(labels.values()), marker=dict(symbol='circle', size=10, color='lightblue'), textposition=\"top center\")\n",
    "    \n",
    "    layout = go.Layout(title=title, scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'), showlegend=False)\n",
    "    \n",
    "    fig = go.Figure(data=[trace_edges, trace_nodes], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "model_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('GPT-2', 'gpt2'),\n",
    "        ('DistilBERT', 'distilbert-base-uncased'),\n",
    "        ('Bert', 'bert-base-uncased'),\n",
    "        ('RoBERTa', 'roberta-base'),\n",
    "        ('aleph-BeRT', 'onlplab/alephbert-base')\n",
    "    ],\n",
    "    value='gpt2',\n",
    "    description='Model:'\n",
    ")\n",
    "\n",
    "tokenizer, model = load_model(model_dropdown.value)\n",
    "\n",
    "text_input1 = Text(description='Text 1:', value='Quantum information theory is interesting')\n",
    "text_input2 = Text(description='Text 2:', value='Quantum information theory allows us to study attention using entanglement')\n",
    "\n",
    "layer_slider = IntSlider(value=1, min=0, max=model.config.num_hidden_layers - 1, description='Layer:', continuous_update=False)\n",
    "head_slider = IntSlider(value=2, min=0, max=model.config.num_attention_heads - 1, description='Head:', continuous_update=False)\n",
    "\n",
    "threshold_slider = FloatSlider(value=0.05, min=0.00, max=0.5, step=0.001, description='Threshold:', continuous_update=False)\n",
    "\n",
    "def update_plot(threshold, text1, text2, layer, head, model_name):\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    layer_slider.max = model.config.num_hidden_layers - 1\n",
    "    head_slider.max = model.config.num_attention_heads - 1\n",
    "\n",
    "    attention_matrix1 = get_attention_matrix(text1, model, tokenizer, layer, head)\n",
    "    attention_matrix2 = get_attention_matrix(text2, model, tokenizer, layer, head)\n",
    "\n",
    "    persistence1, simplex_tree1, distance_matrix1 = compute_persistence(attention_matrix1)\n",
    "    persistence2, simplex_tree2, distance_matrix2 = compute_persistence(attention_matrix2)\n",
    "\n",
    "    tokens1 = tokenizer.tokenize(text1)\n",
    "    tokens2 = tokenizer.tokenize(text2)\n",
    "\n",
    "    tokens1 = [tokenizer.decode(token_id) for token_id in tokenizer.encode(text1)]\n",
    "    tokens2 = [tokenizer.decode(token_id) for token_id in tokenizer.encode(text2)]\n",
    "\n",
    "    plot_simplicial_complex_3d(simplex_tree1, distance_matrix1, \"Simplicial Complex for Text 1\", threshold, tokens1)\n",
    "    plot_simplicial_complex_3d(simplex_tree2, distance_matrix2, \"Simplicial Complex for Text 2\", threshold, tokens2)\n",
    "\n",
    "interact(update_plot, threshold=threshold_slider, text1=text_input1, text2=text_input2, layer=layer_slider, head=head_slider, model_name=model_dropdown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e371ac8-f980-4f78-beb7-38b17c117ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python385jvsc74a57bd0474c67ce7e36ad5731492349411c4ce02ca5c170a680b2d1efe1eb0325e35fe7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
